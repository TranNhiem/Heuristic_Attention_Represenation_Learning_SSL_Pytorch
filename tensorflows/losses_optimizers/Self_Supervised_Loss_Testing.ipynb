{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_Supervised_Loss_Testing",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranNhiem/Heuristic_attention_represenation_learning_ssl/blob/main/tensorflows/losses_optimizers/Self_Supervised_Loss_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa7ZLlQ3BKmp"
      },
      "source": [
        "# Testing Contrastive Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpcyzhve0O_Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry4tGJUABOKW"
      },
      "source": [
        "## Initial_Random_Vector Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG_tWxP7HABV"
      },
      "source": [
        "## Example for Correct Concat the Tensor\n",
        "t1 = [[1, 2, 3], [4, 5, 6]]\n",
        "t2 = [[7, 8, 9], [10, 11, 12]]\n",
        "t_12=tf.concat([t1, t2], 0)\n",
        "tf.print(t_12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnHrKgQb54SN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3692b37-8d44-44fb-d606-7dfadbc2055d"
      },
      "source": [
        "hidden_1= tf.random.uniform(\n",
        "    shape=(32,512), minval=0, maxval=1, dtype=tf.dtypes.float32, seed=None, name=None)\n",
        "hidden_2= tf.random.uniform(\n",
        "    shape=(32,512), minval=0, maxval=1, dtype=tf.dtypes.float32, seed=None, name=None)\n",
        "hidden_1_2=tf.concat((hidden_1, hidden_2),axis=-2)\n",
        "hidden_1_2.shape\n",
        "print(\"Batch_size_Hidden_rep1\",hidden_1.shape)\n",
        "print(\"Batch_size_Hidden_rep2\",hidden_2.shape)\n",
        "print(\"Double_Batch_size_Hidden_rep1_and_rep2\",hidden_1_2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch_size_Hidden_rep1 (32, 512)\n",
            "Batch_size_Hidden_rep2 (32, 512)\n",
            "Double_Batch_size_Hidden_rep1_and_rep2 (64, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWZGHL-DBUZs"
      },
      "source": [
        "## Contrastive Loss Version 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJSgd4ql0O9N"
      },
      "source": [
        "def get_negative_mask(batch_size):\n",
        "    # return a mask that removes the similarity score of equal/similar images\n",
        "    # Ensure distinct pair of image get their similarity scores\n",
        "    # passed as negative examples\n",
        "    batch_size= batch_size.numpy()\n",
        "    negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
        "    for i in range(batch_size):\n",
        "        negative_mask[i, i] = 0\n",
        "        negative_mask[i, i+batch_size] = 0\n",
        "\n",
        "    return tf.constant(negative_mask)\n",
        "\n",
        "def nt_xent_asymetrize_loss_v2(p, z, temperature):  # negative_mask\n",
        "    # L2 Norm\n",
        "    batch_size = tf.shape(p)[0]\n",
        "    batch_size=tf.cast(batch_size, tf.int32)\n",
        "    labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n",
        "    masks = tf.one_hot(tf.range(batch_size), batch_size)\n",
        "\n",
        "    p_l2 = tf.math.l2_normalize(p, axis=1)\n",
        "    z_l2 = tf.math.l2_normalize(z, axis=1)\n",
        "\n",
        "    # Cosine Similarity distance loss\n",
        "\n",
        "    # pos_loss = consie_sim_1d(p_l2, z_l2)\n",
        "    pos_loss = tf.matmul(tf.expand_dims(p_l2, 1), tf.expand_dims(z_l2, 2))\n",
        "\n",
        "    pos_loss = (tf.reshape(pos_loss, (batch_size, 1)))/temperature\n",
        "\n",
        "    negatives = tf.concat([p_l2, z_l2], axis=0)\n",
        "    # Mask out the positve mask from batch of Negative sample\n",
        "    negative_mask = get_negative_mask(batch_size)\n",
        "    print(negative_mask)\n",
        "    \n",
        "    loss = 0\n",
        "    for positives in [p_l2, z_l2]:\n",
        "\n",
        "        # negative_loss = cosine_sim_2d(positives, negatives)\n",
        "        negative_loss = tf.tensordot(tf.expand_dims(\n",
        "            positives, 1), tf.expand_dims(tf.transpose(negatives), 0), axes=2)\n",
        "        l_labels = tf.zeros(batch_size, dtype=tf.float32)\n",
        "        l_neg = tf.boolean_mask(negative_loss, negative_mask)\n",
        "\n",
        "        l_neg = tf.reshape(l_neg, (batch_size, -1))\n",
        "        l_neg /= temperature\n",
        "\n",
        "        logits = tf.concat([pos_loss, l_neg], axis=1)  # [N, K+1]\n",
        "        tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        loss_ = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
        "        loss += loss_(y_pred=logits, y_true=l_labels)\n",
        "    batch_size=tf.cast(batch_size, tf.float32)\n",
        "    \n",
        "    loss = loss/(2*batch_size)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOXuW5Hj0O5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77869924-b8bd-4ce1-9c8f-b48a3c5ef404"
      },
      "source": [
        "loss=nt_xent_asymetrize_loss_v2(hidden_1, hidden_2, temperature=0.5)\n",
        "tf.print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[False  True  True ...  True  True  True]\n",
            " [ True False  True ...  True  True  True]\n",
            " [ True  True False ...  True  True  True]\n",
            " ...\n",
            " [ True  True  True ... False  True  True]\n",
            " [ True  True  True ...  True False  True]\n",
            " [ True  True  True ...  True  True False]], shape=(32, 64), dtype=bool)\n",
            "4.14318657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToB9J6NhBYqM"
      },
      "source": [
        "## Contrastive Loss Version 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS5ZWZNR06Nq"
      },
      "source": [
        "def nt_xent_asymetrize_loss(z,  temperature):\n",
        "\n",
        "    # Feeding data (ALready stack two version Augmented Image)[2*bs, 128]\n",
        "    z = tf.math.l2_normalize(z, axis=1)\n",
        "\n",
        "    similarity_matrix = tf.matmul(z, z, transpose_b=True)  # pairwise similarity\n",
        "    similarity = tf.exp(similarity_matrix / temperature)\n",
        "\n",
        "    ij_indices = tf.reshape(tf.range(z.shape[0]), shape=[-1, 2])\n",
        "    ji_indices = tf.reverse(ij_indices, axis=[1])\n",
        "\n",
        "    #[[0, 1], [1, 0], [2, 3], [3, 2], ...]\n",
        "    positive_indices = tf.reshape(tf.concat(\n",
        "        [ij_indices, ji_indices], axis=1), shape=[-1, 2])  # Indice positive pair\n",
        "    # --> Output N-D array\n",
        "    numerator = tf.gather_nd(similarity, positive_indices)\n",
        "    # 2N-1 (sample)\n",
        "    # mask that discards self-similarity\n",
        "    negative_mask = 1 - tf.eye(z.shape[0])\n",
        "    print(negative_mask)\n",
        "    print(negative_mask.shape)\n",
        "\n",
        "    # compute sume across dimensions of Tensor (Axis is important in this case)\n",
        "    # None sum all element scalar, 0 sum all the row, 1 sum all column -->1D metric\n",
        "    denominators = tf.reduce_sum(\n",
        "        tf.multiply(negative_mask, similarity), axis=1)\n",
        "\n",
        "    losses = -tf.math.log(numerator/denominators)\n",
        "    return tf.reduce_mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjjgBD9b8mND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b34784-88e0-4544-8b64-f6b73a17105f"
      },
      "source": [
        "loss= nt_xent_asymetrize_loss(hidden_1_2, temperature=0.5)\n",
        "tf.print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 1. 1. 1.]\n",
            " [1. 1. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 0. 1. 1.]\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]], shape=(64, 64), dtype=float32)\n",
            "(64, 64)\n",
            "4.14759636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnlidINBBdfm"
      },
      "source": [
        "## Contrastive Loss Version 3 Original Simclr Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TikYcFv06K_"
      },
      "source": [
        "def add_contrastive_loss(hidden,LARGE_NUM,\n",
        "                         hidden_norm=True,\n",
        "                         temperature=0.5,\n",
        "                         weights=1.0, ):\n",
        "\n",
        "  # Get (normalized) hidden1 and hidden2.\n",
        "  if hidden_norm:\n",
        "    hidden = tf.math.l2_normalize(hidden, -1)\n",
        "  hidden1, hidden2 = tf.split(hidden, 2, 0)\n",
        "  batch_size = tf.shape(hidden1)[0]\n",
        "  # Gather hidden1/hidden2 across replicas and create local labels.\n",
        "\n",
        "  hidden1_large = hidden1\n",
        "  hidden2_large = hidden2\n",
        "  labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n",
        "  masks = tf.one_hot(tf.range(batch_size), batch_size)\n",
        "\n",
        "  logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=True) / temperature\n",
        "  logits_aa = logits_aa - masks * LARGE_NUM\n",
        "  logits_bb = tf.matmul(hidden2, hidden2_large, transpose_b=True) / temperature\n",
        "  logits_bb = logits_bb - masks * LARGE_NUM\n",
        "  logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=True) / temperature\n",
        "  logits_ba = tf.matmul(hidden2, hidden1_large, transpose_b=True) / temperature\n",
        "\n",
        "  loss_a = tf.nn.softmax_cross_entropy_with_logits(\n",
        "      labels, tf.concat([logits_ab, logits_aa], 1))\n",
        "  loss_b = tf.nn.softmax_cross_entropy_with_logits(\n",
        "      labels, tf.concat([logits_ba, logits_bb], 1))\n",
        "  \n",
        "  ## Original Simclr LOSS \n",
        "  loss = tf.reduce_mean(loss_a + loss_b)\n",
        "  ## Sugest divide to 2 \n",
        "  loss/=2\n",
        "  return loss, logits_ab, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-K8XRDM3IyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e461979-6333-4b3d-bf93-e09e7102c943"
      },
      "source": [
        "LARGE_NUM=1e-9\n",
        "loss, logits_ab, labels= add_contrastive_loss(hidden_1_2, LARGE_NUM=LARGE_NUM)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is loss value\")\n",
        "tf.print(loss)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is logit output\")\n",
        "tf.print(logits_ab)\n",
        "print(logits_ab.shape)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is label positive ammong Negative batch\")\n",
        "tf.print(labels)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________\n",
            "this is loss value\n",
            "4.16883945\n",
            "____________________\n",
            "this is logit output\n",
            "[[1.4950105 1.56212342 1.51733971 ... 1.47297335 1.53903294 1.46565735]\n",
            " [1.47219634 1.51781917 1.46665359 ... 1.46836185 1.50715327 1.51138663]\n",
            " [1.44336891 1.48364687 1.46776772 ... 1.45604682 1.47696936 1.49555314]\n",
            " ...\n",
            " [1.5380224 1.50131226 1.51715374 ... 1.46726298 1.5188067 1.48607206]\n",
            " [1.51150751 1.48858261 1.48520684 ... 1.48390555 1.51678705 1.45680428]\n",
            " [1.47560918 1.55504358 1.49005628 ... 1.4609375 1.49416399 1.46781635]]\n",
            "(32, 32)\n",
            "____________________\n",
            "this is label positive ammong Negative batch\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(32, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Obf6w7f_507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79aaa83b-bd04-4ca8-a5b2-95d78f66ac29"
      },
      "source": [
        "LARGE_NUM=1e-2\n",
        "loss, logits_ab, labels= add_contrastive_loss(hidden_1_2, LARGE_NUM=LARGE_NUM)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is loss value\")\n",
        "tf.print(loss)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is logit output\")\n",
        "tf.print(logits_ab)\n",
        "print(logits_ab.shape)\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is label positive ammong Negative batch\")\n",
        "tf.print(labels)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________\n",
            "this is loss value\n",
            "4.16858721\n",
            "____________________\n",
            "this is logit output\n",
            "[[1.4950105 1.56212342 1.51733971 ... 1.47297335 1.53903294 1.46565735]\n",
            " [1.47219634 1.51781917 1.46665359 ... 1.46836185 1.50715327 1.51138663]\n",
            " [1.44336891 1.48364687 1.46776772 ... 1.45604682 1.47696936 1.49555314]\n",
            " ...\n",
            " [1.5380224 1.50131226 1.51715374 ... 1.46726298 1.5188067 1.48607206]\n",
            " [1.51150751 1.48858261 1.48520684 ... 1.48390555 1.51678705 1.45680428]\n",
            " [1.47560918 1.55504358 1.49005628 ... 1.4609375 1.49416399 1.46781635]]\n",
            "(32, 32)\n",
            "____________________\n",
            "this is label positive ammong Negative batch\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(32, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjvR3-8xDRVy"
      },
      "source": [
        "## Contrastive Loss Keras Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wjR7BTg3cbJ"
      },
      "source": [
        "def nt_xent_symmetrize_keras(p, z, temperature):\n",
        "    # cosine similarity the dot product of p,z two feature vectors\n",
        "    x_i = tf.math.l2_normalize(p, axis=1)\n",
        "    x_j = tf.math.l2_normalize(z, axis=1)\n",
        "    similarity = (tf.matmul(x_i, x_j, transpose_b=True)/temperature)\n",
        "    # the similarity from the same pair should be higher than other views\n",
        "    batch_size = tf.shape(p)[0]  # Number Image within batch\n",
        "    contrastive_labels = tf.range(batch_size)\n",
        "\n",
        "    # Simlarilarity treat as logic input for Cross Entropy Loss\n",
        "    # Why we need the Symmetrized version Here??\n",
        "    loss_1_2 = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        contrastive_labels, similarity, from_logits=True,)  # reduction=tf.keras.losses.Reduction.SUM\n",
        "    loss_2_1 = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        contrastive_labels, tf.transpose(similarity), from_logits=True, )\n",
        "    loss=(loss_1_2 + loss_2_1) / 2\n",
        "    avarage_loss= tf.reduce_mean(loss)\n",
        "    return loss, avarage_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_LVM6rG3cYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636fb25f-a7e0-4cf5-dbdb-777f485ff665"
      },
      "source": [
        "loss,avarage_loss = nt_xent_symmetrize_keras(hidden_1, hidden_2, temperature=0.5)\n",
        "print(\"____________________\")\n",
        "print(\"this is loss for each Image\")\n",
        "tf.print(loss)\n",
        "loss.shape\n",
        "\n",
        "print(\"____________________\")\n",
        "print(\"this is reduce_sum loss for each batch\")\n",
        "tf.print(avarage_loss)\n",
        "avarage_loss.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________\n",
            "this is loss for each Image\n",
            "[3.47819 3.45527601 3.49630141 ... 3.4932723 3.45434523 3.49010658]\n",
            "____________________\n",
            "this is reduce_sum loss for each batch\n",
            "3.46571207\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KPjQCry3cVh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGXZKpyWFgNL"
      },
      "source": [
        "# Binary Contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdVq5Se3cPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0279a804-7a82-437d-96f8-b129c1e34bf4"
      },
      "source": [
        "def binary_mask_nt_xent_asymetrize_loss(v1_object,v2_object,v1_background, v2_background, alpha = 1, temperature = 1):  # negative_mask\n",
        "    # L2 Norm\n",
        "    batch_size = tf.shape(v1_object)[0]\n",
        "    v1_object = tf.math.l2_normalize(v1_object, -1)\n",
        "    v2_object = tf.math.l2_normalize(v2_object, -1)\n",
        "    v1_background = tf.math.l2_normalize(v1_background, -1)\n",
        "    v2_background = tf.math.l2_normalize(v2_background, -1)\n",
        "\n",
        "    INF = 1e9\n",
        "\n",
        "    labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n",
        "    masks = tf.one_hot(tf.range(batch_size), batch_size)\n",
        "\n",
        "    #Object feature  dissimilar\n",
        "    logits_o_aa = tf.matmul(v1_object, v1_object, transpose_b=True) / temperature\n",
        "    logits_o_aa = logits_o_aa - masks * INF# remove the same samples\n",
        "    logits_o_bb = tf.matmul(v2_object, v2_object, transpose_b=True) / temperature\n",
        "    logits_o_bb = logits_o_bb - masks * INF# remove the same samples\n",
        "\n",
        "    #bject and background feature  dissimilar\n",
        "    logits_b_aa = tf.matmul(v1_object, v1_background, transpose_b=True) / temperature\n",
        "    # logits_b_aa = logits_b_aa - masks * INF# remove the same samples\n",
        "    logits_b_bb = tf.matmul(v2_object, v2_background, transpose_b=True) / temperature\n",
        "    # logits_b_bb = logits_b_bb - masks * INF# remove the same samples\n",
        "\n",
        "    #Object feature  similar\n",
        "    logits_o_ab = tf.matmul(v1_object, v2_object, transpose_b=True) / temperature\n",
        "    logits_o_ba = tf.matmul(v2_object, v1_object, transpose_b=True) / temperature\n",
        "    #background feature  similar\n",
        "    logits_b_ab = tf.matmul(v1_background, v2_background, transpose_b=True) / temperature\n",
        "    logits_b_ba = tf.matmul(v2_background, v1_background, transpose_b=True) / temperature\n",
        "\n",
        "    loss_a = tf.nn.softmax_cross_entropy_with_logits( labels, tf.concat([alpha * logits_o_ab + (1-alpha)*logits_b_ab, alpha * logits_o_aa + alpha * logits_b_aa], 1))\n",
        "    loss_b = tf.nn.softmax_cross_entropy_with_logits( labels, tf.concat([alpha * logits_o_ba + (1-alpha)*logits_b_ba, alpha * logits_o_bb + alpha * logits_b_bb], 1))\n",
        "    # loss_a = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    #     labels, tf.concat([logits_o_ab, logits_o_aa], 1))\n",
        "    # loss_b = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    #     labels, tf.concat([logits_o_ba, logits_o_bb], 1))\n",
        "    # print(loss_a)\n",
        "    # print(loss_b)\n",
        "    loss = tf.reduce_mean(loss_a + loss_b)\n",
        "    loss = loss/2.0\n",
        "\n",
        "    return loss\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    v1_object = tf.random.uniform(shape = [32,256], maxval=1, dtype=tf.float32)\n",
        "    v2_object = tf.random.uniform(shape = [32,256], maxval=1, dtype=tf.float32)\n",
        "    v1_background = tf.random.uniform(shape = [32,256], maxval=1, dtype=tf.float32)\n",
        "    v2_background = tf.random.uniform(shape = [32,256], maxval=1, dtype=tf.float32)\n",
        "    loss = binary_mask_nt_xent_asymetrize_loss(v1_object,v2_object,v1_background, v2_background)\n",
        "\n",
        "    print(\"loss : \",end = \"\")\n",
        "    tf.print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 4.57873726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74efsZaD3cL-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB67US8N3cJN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}